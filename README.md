# TIE–Dialog Region-Based Pilot Evaluation 

This repository contains the full replication package for a region-based pilot evaluation of rupture and repair detection using TIE–Dialog. The study tests whether event regions derived directly from a minimal coherence signal C(t) overlap with human-annotated rupture and repair regions beyond chance, using temporal overlap metrics and permutation baselines.

The core methodological contribution is the use of region-based evaluation (temporal windows) rather than strict point-level matching, which is more appropriate given the temporal uncertainty of human event labeling. In addition, the replication package includes a lag-scan permutation baseline (max-over-lags) to control for lag-selection effects when scanning temporal offsets.

What this repo contains
Notebook (main replication artifact)

notebook/region_evaluation.ipynb

Input data

data/tie_dialog_6_dialogues_combined.csv (dialogue turns + coherence signal C(t))

data/valleys_peaks_final_results.xlsx (human rupture/repair point annotations)

Outputs (generated by the notebook)

results/per_dialogue_model_vs_consensus_optionA_EN.csv

results/per_dialogue_IAA_optionA_EN.csv

results/per_dialogue_best_lag_optionA_EN.csv

results/per_dialogue_baselines_optionA_EN.csv

results/ALL_RESULTS_optionA_EN.xlsx

results/region_metrics_report_optionA_EN.pdf

Method summary
Human annotation processing

Five annotators labeled rupture (valley) and repair-like (peak) points.

Each point at turn t is converted into a fixed temporal window:

[t−2, t+2]

A consensus reference is defined as turns covered by at least:

K = 3/5 annotators

Model event extraction (from the signal only)

Model events are not taken from any explicit system labels. Instead, they are extracted directly from the coherence dynamics using pre-specified, dialogue-relative criteria (Option A):

Repair-like regions

C(t) ≥ Q₀.₈₅(C)

Rupture-like regions

dC(t) = C(t) − C(t−1)

dC(t) ≤ Q₀.₁₅(dC)

All quantiles are computed independently per dialogue.

Evaluation metrics

Intersection-over-Union (IoU) on temporal masks

overlap-based F1 on temporal masks

mask-based precision and recall

Permutation baselines

Two permutation baselines are included:

(1) Random-region baseline at k = 0
Random regions are generated by preserving the model’s:

number of segments

segment lengths

Segments are placed uniformly across the dialogue timeline, merged if overlapping, and compared against consensus masks. The notebook runs B = 2000 permutations and reports p-values for IoU and F1.

(2) Lag-scan permutation baseline (max-over-lags)
Because scanning lags and reporting the best alignment introduces a selection effect, we also compute a lag-aware baseline. For each permutation:

model segments are randomized (same count and lengths)

the same lag scan is performed over k ∈ [−5, +5]

the maximum overlap score across lags (max-over-lags) is recorded

Observed best-lag scores are evaluated against this null distribution, yielding p_best.

Lag scan (precursor alignment; exploratory)

To explore whether model regions systematically precede or follow human-labeled regions, the notebook performs a lag scan:

k ∈ [−5, +5]

reports the best lag k* maximizing IoU (and the corresponding F1)

Negative best lags indicate that overlap improves when model regions are shifted earlier in time. This pattern is evaluated using the lag-scan permutation baseline described above.
# How to run (Google Colab)

Open the notebook in Colab:

Upload notebook/region_evaluation.ipynb to Colab, or open it directly from GitHub.

Upload the two input files when prompted:

data/tie_dialog_6_dialogues_combined.csv

data/valleys_peaks_final_results.xlsx

Run all cells from top to bottom.

The notebook will generate all output tables and the PDF report automatically.

Expected outputs

After running, you should obtain:

Per-dialogue overlap results (model vs consensus)

Human–human agreement (IAA)

Random-region baseline results (IoU/F1 + p-values)

Best-lag results (k* and overlap at k*)

A PDF report summarizing the full evaluation

# Citation

If you use this repository or its evaluation protocol in academic work, please cite the associated preprint and/or the Zenodo release (DOI will be provided in the repository release).

# License

This project is licensed under the Apache License 2.0. See the LICENSE file for details.

# Contact

Adolfo J. Céspedes Jiménez - a.j.cespedes.jimenez(AT)student.rug.nl
(University of Groningen)
